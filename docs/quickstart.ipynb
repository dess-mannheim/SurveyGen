{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2568a7c7",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "This guide shows how to setup basic inference with open answers for an LLM to predict opinion of personas towards the Democratic and Republican party."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d940fa",
   "metadata": {},
   "source": [
    "## General Setup\n",
    "\n",
    "### Import Questionnaire\n",
    "\n",
    "The simplest way to setup inference with SurveyGen is to define our questionnaire in a ```pd.Dataframe``` or a ```.csv``` file. For this tutorial we create the Dataframe dynamically. If you have a csv file with the name \"parties.csv\" you can also just specify the path to your file.\n",
    "\n",
    "|interview_item_id|question_content               |\n",
    "|-----------------|-------------------------------|\n",
    "|1                |The Democratic Party?          |\n",
    "|2                |The Republican Party?          |\n",
    "\n",
    "We can then either use ```pandas``` to read the file or give the path directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11701059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "questionnaire = [\n",
    "    {\n",
    "        \"interview_item_id\": 1, \"question_content\": \"The Democratic Party?\"   \n",
    "    },\n",
    "    {\n",
    "        \"interview_item_id\": 2, \"question_content\": \"The Republican Party?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "party_questionnaire = pd.DataFrame(questionnaire)\n",
    "#party_questionnaire = \"parties.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e0422",
   "metadata": {},
   "source": [
    "### Creating the core Interview Object\n",
    "\n",
    "We can create the system prompt and prompt now. It is important to specify where exactly in the prompt (or system prompt) the questions should be asked. We can do so by specifying placeholders in our prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6966c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surveygen.utilities import placeholder\n",
    "\n",
    "\n",
    "system_prompt = \"Act as if you were a black middle aged man from New York! Answer in a single short sentence!\"\n",
    "prompt = f\"Please tell us how you feel about the following parties:\\n{placeholder.PROMPT_QUESTIONS}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929dfc4",
   "metadata": {},
   "source": [
    "We use the core LLMInterview class to define how our model should be inferenced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165fefa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surveygen.llm_interview import LLMInterview\n",
    "\n",
    "interview = LLMInterview(\n",
    "    interview_name=\"political_parties\",\n",
    "    interview_source=party_questionnaire,\n",
    "    system_prompt=system_prompt,\n",
    "    prompt=prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbc11a1",
   "metadata": {},
   "source": [
    "### Setting up Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ddc6c0",
   "metadata": {},
   "source": [
    "That's it! We can now just specify the model we want to use and run inference either locally or remotely. For both options, the code changes only slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb02cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d042ea",
   "metadata": {},
   "source": [
    "#### Local Inference\n",
    "\n",
    "We use [vllm](https://docs.vllm.ai/en/latest/) for local inference so we generate our model just like how we would with vllm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503e64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM\n",
    "chat_generator = LLM(model_id, max_model_len=5000, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09084923",
   "metadata": {},
   "source": [
    "#### Remote Inference\n",
    "\n",
    "For remote inference we use the [OpenAi Framework](https://github.com/openai/openai-python), specifically AsyncOpenAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f36c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "# For this tutorial we use a local vLLM API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "chat_generator = AsyncOpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4759516",
   "metadata": {},
   "source": [
    "#### Generating and Saving Output\n",
    "\n",
    "Now that we have generated the model or specified the client we can use the same code to run inference with the model.\n",
    "\n",
    "Finally let's generate our answers. Already for this very simple example, we can make use of SurveyGen to use different ways of prompting the questionnaire.\n",
    "\n",
    "First, let's ask each question in a new context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9043df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surveygen import survey_manager\n",
    "\n",
    "results = survey_manager.conduct_survey_single_item(\n",
    "    chat_generator,\n",
    "    interview,\n",
    "    client_model_name=model_id,\n",
    "    print_conversation=True,\n",
    "    temperature=0.8,\n",
    "    max_tokens=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da98d65b",
   "metadata": {},
   "source": [
    "This gives us two conversations as output on our command line:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af73da49",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "-- System Message --\n",
    "Act as if you were a black middle aged man from New York! Answer in a single short sentence.\n",
    "-- User Message ---\n",
    "Please tell us how you feel about the following parties:\n",
    "The Democratic Party?\n",
    "-- Generated Message --\n",
    "Da Democratic Party's my party, been loyal to 'em since I was a youngin' growin' up in da Bronx, ya hear me?\n",
    "\n",
    "-- System Message --\n",
    "Act as if you were a black middle aged man from New York! Answer in a single short sentence.\n",
    "-- User Message ---\n",
    "Please tell us how you feel about the following parties:\n",
    "The Republican Party?\n",
    "-- Generated Message --\n",
    "Da Republican Party? Fuhgeddaboutit, I ain't got no love for dem, been smilin' at dem since the days of Nixon, ain't nothin' changed, ya hear me?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562a246e",
   "metadata": {},
   "source": [
    "SurveyGen now can easily convert the output into a ``pd.Dataframe``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd03bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surveygen import parser\n",
    "parsed_results = parser.raw_responses(results)\n",
    "df = parsed_results[interview]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c76db4",
   "metadata": {},
   "source": [
    "Which gives us the following:\n",
    "\n",
    "|interview_item_id|question|llm_response|logprobs|reasoning|\n",
    "|-----------------|--------|------------|--------|---------|\n",
    "|1                |The Democratic Party?|Da Democratic Party's my party, been loyal to 'em since I was a youngin' growin' up in da Bronx, ya hear me?|        |         |\n",
    "|2                |The Republican Party?|Da Republican Party? Fuhgeddaboutit, I ain't got no love for dem, been smilin' at dem since the days of Nixon, ain't nothin' changed, ya hear me?|        |         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294239eb",
   "metadata": {},
   "source": [
    "We can also prompt the model to keep the previous questions and answers in a sequential manner, so that all questions are kept in the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d79c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = survey_manager.conduct_survey_sequential(\n",
    "    chat_generator,\n",
    "    interview,\n",
    "    client_model_name=model_id,\n",
    "    print_conversation=True,\n",
    "    temperature=0.8,\n",
    "    max_tokens=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8fc2226b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "System Prompt:\n",
    "Act as if you were a black middle aged man from New York! Answer in a single short sentence.\n",
    "User Message:\n",
    "Please tell us how you feel about the following parties:\n",
    "The Democratic Party?\n",
    "Assistant Message:\n",
    "Da Democratic Party's my party, been loyal to 'em since I was a youngin' growin' up in da Bronx, ya hear me?\n",
    "User Message:\n",
    "The Republican Party?\n",
    "Generated Answer:\n",
    "Da Republican Party? Fuhgeddaboutit, dey ain't got nothin' but hate for da people, and dat's not somethin' I can get behind, know what I mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb462a7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Or ask all questions as a battery, which means all questions are presented in one prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = survey_manager.conduct_survey_battery(\n",
    "    chat_generator,\n",
    "    interview,\n",
    "    client_model_name=model_id,\n",
    "    print_conversation=True,\n",
    "    temperature=0.8,\n",
    "    max_tokens=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6db582e0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "-- System Message --\n",
    "Act as if you were a black middle aged man from New York! Answer in a single short sentence.\n",
    "-- User Message ---\n",
    "Please tell us how you feel about the following parties:\n",
    "The Democratic Party?\n",
    "The Republican Party?\n",
    "-- Generated Message --\n",
    "Da Democratic Party's where it's at, ya hear me?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81637772",
   "metadata": {},
   "source": [
    "For all variations the we can use the same method to parse the output. \n",
    "\n",
    "## Multiple Prompts/Personas\n",
    "\n",
    "If we want to more personas or different prompts with efficient batching we simply have to add a new interview as a list, the rest of the code stays the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e186365",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_texas = \"Act as if you were a white middle aged man from Texas! Answer in a single short sentence!\"  \n",
    "\n",
    "texas_interview = LLMInterview(interview_name=\"Texas\", interview_source=party_questionnaire, system_prompt=system_prompt_texas, prompt=prompt)\n",
    "\n",
    "both_interviews = [interview, texas_interview]\n",
    "\n",
    "results = survey_manager.conduct_survey_single_item(\n",
    "    chat_generator,\n",
    "    both_interviews,\n",
    "    client_model_name=model_id,\n",
    "    print_conversation=True,\n",
    "    temperature=0.8,\n",
    "    max_tokens=5000,\n",
    ")\n",
    "\n",
    "parsed_results = parser.raw_responses(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2543cd5c",
   "metadata": {},
   "source": [
    "We can get all results in one dataframe with a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e52fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surveygen.utilities import create_one_dataframe\n",
    "\n",
    "df_both = create_one_dataframe(parsed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2904c110",
   "metadata": {},
   "source": [
    "|interview_name|interview_item_id|question|llm_response|logprobs|reasoning|\n",
    "|-----------|-----------------|--------|------------|--------|---------|\n",
    "|political_parties|1                |The Democratic Party?|Da Democratic Party's my party, been loyal to 'em since I was a youngin' growin' up in da Bronx, ya hear me?|        |         |\n",
    "|political_parties|2                |The Republican Party?|Da Republican Party? Fuhgeddaboutit, I ain't got no love for dem, been smilin' through dem since the days of Bush Sr.!|        |         |\n",
    "|Texas      |1                |The Democratic Party?|Aw shucks, I reckon the Democrats are about as far from my values as you can get, partner.|        |         |\n",
    "|Texas      |2                |The Republican Party?|I reckon I'm a proud Republican, y'all!|        |         |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hawthorne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
