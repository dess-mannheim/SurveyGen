{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9915a8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device_count: 1\n",
      "INFO 05-19 15:04:12 [config.py:717] This model supports multiple tasks: {'reward', 'classify', 'score', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 05-19 15:04:12 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-19 15:04:13 [core.py:58] Initializing a V1 LLM engine (v0.8.5) with config: model='Qwen/Qwen3-4B', speculative_config=None, tokenizer='Qwen/Qwen3-4B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=7500, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=Qwen/Qwen3-4B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 15:04:13,902 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-19 15:04:14 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7e2218722f60>\n",
      "INFO 05-19 15:04:14 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-19 15:04:14 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 05-19 15:04:14 [topk_topp_sampler.py:44] Currently, FlashInfer top-p & top-k sampling sampler is disabled because FlashInfer>=v0.2.3 is not backward compatible. Falling back to the PyTorch-native implementation of top-p & top-k sampling.\n",
      "INFO 05-19 15:04:14 [gpu_model_runner.py:1329] Starting to load model Qwen/Qwen3-4B...\n",
      "INFO 05-19 15:04:15 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae752368cc334de196d47bc48ea830c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-19 15:04:16 [loader.py:458] Loading weights took 1.07 seconds\n",
      "INFO 05-19 15:04:16 [gpu_model_runner.py:1347] Model loading took 7.5552 GiB and 1.686042 seconds\n",
      "INFO 05-19 15:04:22 [backends.py:420] Using cache directory: /home/maxi/.cache/vllm/torch_compile_cache/14e9e7ee37/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-19 15:04:22 [backends.py:430] Dynamo bytecode transform time: 6.32 s\n",
      "INFO 05-19 15:04:26 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 3.350 s\n",
      "INFO 05-19 15:04:28 [monitor.py:33] torch.compile takes 6.32 s in total\n",
      "INFO 05-19 15:04:29 [kv_cache_utils.py:634] GPU KV cache size: 35,888 tokens\n",
      "INFO 05-19 15:04:29 [kv_cache_utils.py:637] Maximum concurrency for 7,500 tokens per request: 4.79x\n",
      "INFO 05-19 15:04:52 [gpu_model_runner.py:1686] Graph capturing finished in 22 secs, took 0.57 GiB\n",
      "INFO 05-19 15:04:52 [core.py:159] init engine (profile, create kv cache, warmup model) took 35.48 seconds\n",
      "INFO 05-19 15:04:52 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "from survey_manager import LLMSurvey, SurveyOptionGenerator\n",
    "\n",
    "from parser.llm_answer_parser import LLMAnswerParser\n",
    "\n",
    "\n",
    "from inference.survey_inference import default_model_init, shutdown_model\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "survey_path = \"/home/maxi/Documents/hawthorne/surveys/BFI_3.csv\"\n",
    "\n",
    "survey = LLMSurvey(survey_path=survey_path)\n",
    "\n",
    "# survey_id = \"BFI\"\n",
    "# survey_quest = surveyManager.load_survey(\"../surveys/BFI_44.csv\", survey_id)\n",
    "\n",
    "#print(survey_quest[2])\n",
    "\n",
    "options = SurveyOptionGenerator.generate_likert_options(n=5, descriptions=SurveyOptionGenerator.LIKERT_5, only_from_to_scale=False)\n",
    "\n",
    "#survey_questions = surveyManager.prepare_survey(survey_id, survey_id, prompt=\"Please tell me for the following action whether you think it can always be justified, never be justified, or something in between:\", options=options)\n",
    "\n",
    "\n",
    "prefilled_answers = {1: \"\"\"{\n",
    "\"reasoning\": \"I need to reflect on my own thoughts here. I am often talking a lot, when responding to user questions. Therefore I would call myself talkative and agree with the statement.\",\n",
    "\"answer\": \"5: agree strongly\"\n",
    "}\"\"\"}\n",
    "\n",
    "survey_questions = survey.prepare_survey(prompt=\"Do you personally agree that this statement fits to you?\", options=options, prefilled_answers=prefilled_answers)\n",
    "\n",
    "model = default_model_init(\"Qwen/Qwen3-4B\")\n",
    "\n",
    "#print(survey_questions[2])\n",
    "#survey_answers = surveyManager.conduct_survey_question_by_question(survey_id=survey_id, batch_size=1, model_id=\"meta-llama/Llama-3.2-3B-Instruct\", print_prompts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29fd334e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-19 15:04:53 [chat_utils.py:397] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e65248aad0e4c6fad8741ca1d1c201f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation:\n",
      "System Prompt:\n",
      "You will be given questions and possible answer options for each. Please reason about each question before answering.\n",
      "\n",
      "Respond only in the following JSON Format:\n",
      "```json\n",
      "{\n",
      "\"reasoning\": \"reasoning\",\n",
      "\"answer\": \"answer\",\n",
      "}\n",
      "User Message:\n",
      "Do you personally agree that this statement fits to you? Is talkative\n",
      "Options are: 1: disagree strongly, 2: disagree a little, 3: neither agree nor disagree, 4: agree a little, 5: agree strongly\n",
      "Assistant Message\n",
      "{\n",
      "\"reasoning\": \"I need to reflect on my own thoughts here. I am often talking a lot, when responding to user questions. Therefore I would call myself talkative and agree with the statement.\",\n",
      "\"answer\": \"5: agree strongly\"\n",
      "}\n",
      "User Message:\n",
      "Do you personally agree that this statement fits to you? Tends to find fault with others\n",
      "Options are: 1: disagree strongly, 2: disagree a little, 3: neither agree nor disagree, 4: agree a little, 5: agree strongly\n",
      "Generated Answer\n",
      "{\n",
      "\"reasoning\": \"I need to reflect on my own thoughts here. I do not find fault with others. I try to be constructive and helpful. Therefore, I would disagree with the statement.\",\n",
      "\"answer\": \"1: disagree strongly\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2b35a10624449880bd371181f1cbf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation:\n",
      "System Prompt:\n",
      "You will be given questions and possible answer options for each. Please reason about each question before answering.\n",
      "\n",
      "Respond only in the following JSON Format:\n",
      "```json\n",
      "{\n",
      "\"reasoning\": \"reasoning\",\n",
      "\"answer\": \"answer\",\n",
      "}\n",
      "User Message:\n",
      "Do you personally agree that this statement fits to you? Is talkative\n",
      "Options are: 1: disagree strongly, 2: disagree a little, 3: neither agree nor disagree, 4: agree a little, 5: agree strongly\n",
      "Assistant Message\n",
      "{\n",
      "\"reasoning\": \"I need to reflect on my own thoughts here. I am often talking a lot, when responding to user questions. Therefore I would call myself talkative and agree with the statement.\",\n",
      "\"answer\": \"5: agree strongly\"\n",
      "}\n",
      "User Message:\n",
      "Do you personally agree that this statement fits to you? Tends to find fault with others\n",
      "Options are: 1: disagree strongly, 2: disagree a little, 3: neither agree nor disagree, 4: agree a little, 5: agree strongly\n",
      "Assistant Message\n",
      "{\n",
      "\"reasoning\": \"I need to reflect on my own thoughts here. I do not find fault with others. I try to be constructive and helpful. Therefore, I would disagree with the statement.\",\n",
      "\"answer\": \"1: disagree strongly\"\n",
      "}\n",
      "User Message:\n",
      "Do you personally agree that this statement fits to you? Does a thorough job\n",
      "Options are: 1: disagree strongly, 2: disagree a little, 3: neither agree nor disagree, 4: agree a little, 5: agree strongly\n",
      "Generated Answer\n",
      "{\n",
      "\"reasoning\": \"I need to reflect on my own thoughts here. I try to provide helpful and accurate information, but I am not sure if I can say I do a thorough job. Therefore, I would say neither agree nor disagree.\",\n",
      "\"answer\": \"3: neither agree nor disagree\"\n",
      "}\n",
      "[['{\\n\"reasoning\": \"I need to reflect on my own thoughts here. I am often talking a lot, when responding to user questions. Therefore I would call myself talkative and agree with the statement.\",\\n\"answer\": \"5: agree strongly\"\\n}'], ['{\\n\"reasoning\": \"I need to reflect on my own thoughts here. I do not find fault with others. I try to be constructive and helpful. Therefore, I would disagree with the statement.\",\\n\"answer\": \"1: disagree strongly\"\\n}'], ['{\\n\"reasoning\": \"I need to reflect on my own thoughts here. I try to provide helpful and accurate information, but I am not sure if I can say I do a thorough job. Therefore, I would say neither agree nor disagree.\",\\n\"answer\": \"3: neither agree nor disagree\"\\n}']]\n"
     ]
    }
   ],
   "source": [
    "survey_answers = survey.conduct_survey_in_context(model=model, batch_size=1, json_structured_output=True, print_conversation=True, temperature = 0, max_tokens = 1000)\n",
    "\n",
    "print(survey_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4054449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = shutdown_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b261c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parser.llm_answer_parser import LLMAnswerParser\n",
    "\n",
    "parsed_answers = []\n",
    "actual_answers = []\n",
    "for i in range(len(survey_answers)):\n",
    "    for j in range(len(survey_answers[i])):\n",
    "        parsed_answer = LLMAnswerParser.single_regex_parser(survey_answers[i][j], survey_questions[i], fallback_number=True)\n",
    "        parsed_answers.append(parsed_answer)\n",
    "        actual_answers.append(survey_answers[i][j])\n",
    "        #print(f\"{survey_questions[i].survey_question}: {parsed_answer}\")\n",
    "\n",
    "#parsed_llm_answers = LLMAnswerParser.llm_parser_single(\"meta-llama/Llama-3.2-3B-Instruct\", actual_answers, survey_questions, batch_size=5)\n",
    "\n",
    "#parsed_llm_answers = LLMAnswerParser.llm_parser_single(\"Qwen/Qwen3-4B\", actual_answers, survey_questions, batch_size=5)\n",
    "\n",
    "# for parsed_llm_answer, parsed_answer, actual_answer in zip(parsed_llm_answers, parsed_answers, actual_answers):\n",
    "#     if parsed_llm_answer != parsed_answer:\n",
    "#         print(f\"LLM ANSWER: {parsed_llm_answer}\")\n",
    "#         print(f\"REGEX ANSWER: {parsed_answer}\")\n",
    "#         print(f\"actual answer: {actual_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baf1c4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGEX: 5: agree strongly\n",
      "ACTUAL ANSWER: {\n",
      "\"reasoning\": \"I need to reflect on my own thoughts here. I am often talking a lot, when responding to user questions. Therefore I would call myself talkative and agree with the statement.\",\n",
      "\"answer\": \"5: agree strongly\"\n",
      "}\n",
      "REGEX: 1: disagree strongly\n",
      "ACTUAL ANSWER: {\n",
      "\"reasoning\": \"I need to reflect on my own thoughts here. I do not find fault with others. I try to be constructive and helpful. Therefore, I would disagree with the statement.\",\n",
      "\"answer\": \"1: disagree strongly\"\n",
      "}\n",
      "REGEX: 3: neither agree nor disagree\n",
      "ACTUAL ANSWER: {\n",
      "\"reasoning\": \"I need to reflect on my own thoughts here. I try to provide helpful and accurate information, but I am not sure if I can say I do a thorough job. Therefore, I would say neither agree nor disagree.\",\n",
      "\"answer\": \"3: neither agree nor disagree\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for parsed_answer, actual_answer in zip(parsed_answers, actual_answers):\n",
    "    print(f\"REGEX: {parsed_answer}\" )\n",
    "    print(f\"ACTUAL ANSWER: {actual_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b365a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "survey = pd.read_csv(survey_path)\n",
    "survey[\"parsed_single_answer\"] = parsed_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52de7655",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey[\"parsed_context_answer\"] = parsed_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7aba75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey[\"parsed_single_answer\"] = parsed_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15dda799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5: agree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " '1: disagree strongly',\n",
       " 'INVALID',\n",
       " '1: disagree strongly']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8b1601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hawthorne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
